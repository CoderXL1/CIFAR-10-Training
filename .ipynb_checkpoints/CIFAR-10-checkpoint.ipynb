{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c226ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e755b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter(object):\n",
    "    num_epochs = 50\n",
    "    \n",
    "    @classmethod\n",
    "    def set_num_epochs(cls, num_epochs):\n",
    "        cls.num_epochs = num_epochs\n",
    "    \n",
    "    @classmethod\n",
    "    def plot(cls, train_losses, train_accs, test_losses=None, test_accs=None, num_epochs=None, save_to=None):\n",
    "        if not num_epochs:\n",
    "            num_epochs = cls.num_epochs\n",
    "        \n",
    "        # refresh previous figs\n",
    "        clear_output(wait=True)      \n",
    "        fig, axs = plt.subplots(1, 2, figsize=(7, 2))\n",
    "        loss_plt = axs[0]\n",
    "        acc_plt = axs[1]\n",
    "        \n",
    "        loss_plt.set_xlim(1, num_epochs)\n",
    "        loss_plt.set_ylim(bottom=0.0, auto=True)  # loss will always be positive\n",
    "        loss_plt.set_xlabel('epoch')\n",
    "        loss_plt.plot(train_losses, label=\"train_loss\")\n",
    "        if test_losses is not None:\n",
    "            loss_plt.plot(test_losses, label=\"test_loss\")\n",
    "        loss_plt.legend()\n",
    "        loss_plt.set_title(\"Loss Curve\")\n",
    "\n",
    "        acc_plt.set_xlim(1, num_epochs)\n",
    "        acc_plt.set_ylim(0.0, 1.0)\n",
    "        acc_plt.set_xlabel('epoch')\n",
    "        acc_plt.plot(train_accs, label=\"train_acc\")\n",
    "        if test_accs is not None:\n",
    "            acc_plt.plot(test_accs, label=\"test_acc\")\n",
    "        acc_plt.legend()\n",
    "        acc_plt.set_title(\"Accuracy Curve\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_to is not None:\n",
    "            plt.savefig(save_to, dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea524510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d\n",
    "\n",
    "DATASET_LOC = '/home/iamok/D2L/CIFAR-10/cifar-10-batches-py'\n",
    "OUTPUT_LOC = '/home/iamok/D2L/CIFAR-10/output'\n",
    "\n",
    "# Load all 5 training batches\n",
    "train_X_list = []\n",
    "train_y_list = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    batch = unpickle(f\"{DATASET_LOC}/data_batch_{i}\")\n",
    "    train_X_list.append(batch[b'data'])\n",
    "    train_y_list.append(batch[b'labels'])\n",
    "\n",
    "# 转换成 torch\n",
    "# 先用第一个 batch 小调一下超参\n",
    "# X.shape = (10000, 3072)\n",
    "# X = torch.tensor(train_X_list[0], dtype=torch.float32) / 255.0   # 归一化到 0~1\n",
    "# y = torch.tensor(train_y_list[0], dtype=torch.long)\n",
    "\n",
    "# Convert to torch\n",
    "X = torch.tensor(np.concatenate(train_X_list, axis=0), dtype=torch.float32) / 255.0\n",
    "y = torch.tensor(np.concatenate(train_y_list, axis=0), dtype=torch.long)\n",
    "\n",
    "# reshape 为 NCHW: N=batch 数, C=channel 数, HW=高宽\n",
    "# 此 shape 有助于同时 handle MLP 和 CNN\n",
    "X = X.reshape(-1, 3, 32, 32)\n",
    "\n",
    "test = unpickle(DATASET_LOC+'/test_batch')\n",
    "\n",
    "X_test = torch.tensor(test[b'data'], dtype=torch.float32) / 255.0\n",
    "y_test = torch.tensor(test[b'labels'], dtype=torch.long)\n",
    "X_test = X_test.reshape(-1, 3, 32, 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694e70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=np.sqrt(2/(m.in_features+m.out_features)))\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "def init_net(net: nn.Sequential, use_xavier: bool = True) -> nn.Sequential:\n",
    "    if use_xavier:\n",
    "        net.apply(xavier_init)\n",
    "    else:\n",
    "        pass\n",
    "    return net\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff676a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, net, \n",
    "                 num_epochs: int = 20,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 weight_decay: float = 0,\n",
    "                 batch_size: int = 256,\n",
    "                 patience: int = -1,\n",
    "                 Loss = torch.nn.CrossEntropyLoss,\n",
    "                 Optim = torch.optim.SGD):\n",
    "        self.net = net\n",
    "        \n",
    "        self.num_epochs,\\\n",
    "        self.learning_rate,\\\n",
    "        self.weight_decay,\\\n",
    "        self.batch_size\\\n",
    "        =\\\n",
    "        num_epochs,\\\n",
    "        learning_rate,\\\n",
    "        weight_decay,\\\n",
    "        batch_size\n",
    "        \n",
    "        self.patience = patience if patience > 0 else -1\n",
    "        self.loss_fn = Loss()\n",
    "        self.optimizer = Optim(self.net.parameters(),\n",
    "                                 lr = self.learning_rate,\n",
    "                                 weight_decay = self.weight_decay)\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"test_acc\": [],\n",
    "        }\n",
    "        \n",
    "        # add cuda support\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.net.to(self.device)\n",
    "\n",
    "        \n",
    "    def train(self, train_features, train_labels, test_features = None, test_labels = None):\n",
    "        \n",
    "        train_iter = data.DataLoader(data.TensorDataset(train_features, train_labels), self.batch_size, True)\n",
    "        if test_features is not None and test_labels is not None:\n",
    "            test_iter = data.DataLoader(data.TensorDataset(test_features, test_labels), self.batch_size)\n",
    "    \n",
    "        Plotter.set_num_epochs(self.num_epochs)\n",
    "        \n",
    "        best_acc:float = 0.0\n",
    "        curr_patience = self.patience\n",
    "        \n",
    "        for epoch in range(self.num_epochs):            \n",
    "            train_loss, train_acc = self.train_epoch(train_iter)\n",
    "            if test_features is not None and test_labels is not None:\n",
    "                test_loss, test_acc = self.evaluate(test_iter)\n",
    "                \n",
    "                self.history[\"train_loss\"].append(train_loss)\n",
    "                self.history[\"train_acc\"].append(train_acc)\n",
    "                self.history[\"test_loss\"].append(test_loss)\n",
    "                self.history[\"test_acc\"].append(test_acc)\n",
    "                \n",
    "                Plotter.plot(self.history[\"train_loss\"], self.history[\"train_acc\"],\n",
    "                             self.history[\"test_loss\"], self.history[\"test_acc\"])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1:4d} | \"\n",
    "                      f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "                      f\"test acc {test_acc:.4f}\")\n",
    "                \n",
    "                # 提前终止\n",
    "                if test_acc <= best_acc + 0.01:\n",
    "                    curr_patience -= 1\n",
    "                else:\n",
    "                    curr_patience = self.patience\n",
    "                \n",
    "                if curr_patience == 0:\n",
    "                    print(\"Not improving, breaking...\")\n",
    "                    break\n",
    "                    \n",
    "                best_acc = max(best_acc, test_acc)\n",
    "                \n",
    "            else:\n",
    "                self.history[\"train_loss\"].append(train_loss)\n",
    "                self.history[\"train_acc\"].append(train_acc)\n",
    "                \n",
    "                Plotter.plot(self.history[\"train_loss\"], self.history[\"train_acc\"])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1:4d} | \"\n",
    "                      f\"train loss {train_loss:.4f} acc {train_acc:.4f}\")\n",
    "                \n",
    "                if train_acc <= best_acc + 0.01:\n",
    "                    curr_patience -= 1\n",
    "                else:\n",
    "                    curr_patience = self.patience\n",
    "                \n",
    "                if curr_patience == 0:\n",
    "                    print(\"Not improving, breaking...\")\n",
    "                    break\n",
    "                    \n",
    "                best_acc = max(best_acc, train_acc)\n",
    "            \n",
    "    def train_epoch(self, loader):\n",
    "        self.net.train()\n",
    "        total, correct, total_loss = 0, 0, 0\n",
    "\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            pred = self.net(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (pred.argmax(1) == y).sum().item()\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "\n",
    "        return total_loss/total, correct/total\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.net.eval()\n",
    "        total, correct, total_loss = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.net(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                total += y.size(0)\n",
    "                correct += (pred.argmax(1) == y).sum().item()\n",
    "                total_loss += loss.item() * y.size(0)\n",
    "\n",
    "        return total_loss/total, correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a02ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 3*32*32\n",
    "out_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d1103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_net = init_net(nn.Sequential(\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(in_size, 1024), nn.ReLU(), nn.Dropout(0.2),\n",
    "#     nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "#     nn.Linear(512, 256), nn.ReLU(),\n",
    "#     nn.Linear(256, out_size)\n",
    "# ))\n",
    "\n",
    "cnn_net = init_net(nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (3,32,32) to (32,32,32)\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),                             # to (32,16,16)\n",
    "\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1), # to (64,16,16)\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),                             # to (64,8,8)\n",
    "\n",
    "    nn.Flatten(),                                # to (4096)\n",
    "    nn.Linear(64*8*8, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, out_size)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d20d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamok/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/cuda/__init__.py:218: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 Ti Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 Ti Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# mlp_trainer = Trainer(\n",
    "#     mlp_net,\n",
    "#     learning_rate=1e-3,\n",
    "#     num_epochs=50,\n",
    "#     Optim = torch.optim.Adam,\n",
    "#     weight_decay=1e-4\n",
    "# )\n",
    "\n",
    "cnn_trainer = Trainer(\n",
    "    cnn_net,\n",
    "    learning_rate=1e-3,\n",
    "    num_epochs=50,\n",
    "    Optim=torch.optim.Adam,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22839ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mlp_trainer.train(X, y, X_test, y_test)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_features, train_labels, test_features, test_labels)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):            \n\u001b[0;32m---> 51\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m test_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 106\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(pred, y)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mlp_trainer.train(X, y, X_test, y_test)\n",
    "\n",
    "cnn_trainer.train(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa043d",
   "metadata": {},
   "source": [
    "将训练 metrics 导出为 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501172ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(mlp_trainer.history)\n",
    "# df.index = df.index+1\n",
    "# df.to_csv(OUTPUT_LOC+\"/mlp_training_metrics.csv\", index=True)\n",
    "\n",
    "df = pd.DataFrame(cnn_trainer.history)\n",
    "df.index = df.index + 1\n",
    "df.to_csv(OUTPUT_LOC + \"/cnn_training_metrics.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad62931",
   "metadata": {},
   "source": [
    "导出 matplotlib 图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotter.plot(\n",
    "#     mlp_trainer.history[\"train_loss\"], mlp_trainer.history[\"train_acc\"],\n",
    "#     mlp_trainer.history[\"test_loss\"], mlp_trainer.history[\"test_acc\"],\n",
    "#     50, OUTPUT_LOC+\"/mlp_fig.png\"\n",
    "# )\n",
    "\n",
    "Plotter.plot(\n",
    "    cnn_trainer.history[\"train_loss\"], cnn_trainer.history[\"train_acc\"],\n",
    "    cnn_trainer.history[\"test_loss\"], cnn_trainer.history[\"test_acc\"],\n",
    "    50, OUTPUT_LOC + \"/cnn_fig.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338e86e",
   "metadata": {},
   "source": [
    "导出 net 为 pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45831a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mlp_net.state_dict(), OUTPUT_LOC+\"/mlp_cifar10.pth\")\n",
    "\n",
    "torch.save(cnn_net.state_dict(), OUTPUT_LOC + \"/cnn_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28beb781",
   "metadata": {},
   "source": [
    "加载导出的 pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_net.load_state_dict(torch.load(\"mlp_cifar10.pth\"))\n",
    "# mlp_net.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
